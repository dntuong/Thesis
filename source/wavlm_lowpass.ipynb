{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1DS0-qUGek0HKwN5rmPPELbT64h-ufDIO","timestamp":1687368557220},{"file_id":"1VfEtZPtF_QZR8EkrWMLQ8Zk4Rm6zkNf8","timestamp":1686596737253},{"file_id":"1rp-piCm1hIXnBxAzJwQFb5HVH1npjmcx","timestamp":1683276954233},{"file_id":"1w_hdnvg01KVbxQcfl0HUdKU6grlOTvRL","timestamp":1678340991838},{"file_id":"1-I3BbAG2hTr1r2u9W0oSjlDQ5JSwpN8H","timestamp":1676645769812},{"file_id":"1PIHd_l461oPzlaiC3nxrmYGnUUUMi9-B","timestamp":1675926343722},{"file_id":"1IDd35YxTXU1GzzQCzvi_BZASPvW9Voff","timestamp":1672475923756},{"file_id":"1GmnE4ibsW7fdRXdnQS91_MVFJdAdEcrc","timestamp":1672467463465}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"PZkXqLWNWEKU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687412274299,"user_tz":-420,"elapsed":22580,"user":{"displayName":"Nhattoan Do","userId":"06677995424191825585"}},"outputId":"b56c1d71-d820-46f0-f736-e4cfce77ddcb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["# Original: 1012\n","# current: epoch 48\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["!pip install s3prl -q\n","!pip install hydra-core -q\n","!pip install sentencepiece -q\n","!pip uninstall omegaconf -y\n","!pip install omegaconf\n","!pip install fairseq -q"],"metadata":{"id":"Zz92Nz22SpDM","colab":{"base_uri":"https://localhost:8080/","height":872},"executionInfo":{"status":"ok","timestamp":1687412376454,"user_tz":-420,"elapsed":102158,"user":{"displayName":"Nhattoan Do","userId":"06677995424191825585"}},"outputId":"9ee5752b-fb89-4d84-c688-c4d51241c8f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m952.4/952.4 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for s3prl (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\n","arviz 0.15.1 requires setuptools>=60.0.0, but you have setuptools 59.5.0 which is incompatible.\n","cvxpy 1.3.1 requires setuptools>65.5.1, but you have setuptools 59.5.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hFound existing installation: omegaconf 2.3.0\n","Uninstalling omegaconf-2.3.0:\n","  Successfully uninstalled omegaconf-2.3.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting omegaconf\n","  Using cached omegaconf-2.3.0-py3-none-any.whl (79 kB)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf) (4.9.3)\n","Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf) (6.0)\n","Installing collected packages: omegaconf\n","Successfully installed omegaconf-2.3.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pydevd_plugins"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/9.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/9.6 MB\u001b[0m \u001b[31m168.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","s3prl 0.4.10 requires omegaconf>=2.1.1, but you have omegaconf 2.0.6 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["const_path = '/content/gdrive/MyDrive/'"],"metadata":{"id":"RaESucG0buTw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","import os\n","py_file_location = const_path + '/KLTN/source/wavlm_large'\n","sys.path.append(os.path.abspath(py_file_location))\n","import fairseq\n","import tqdm\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn.metrics import *\n","import scipy.interpolate, scipy.optimize\n","import numpy as np\n","import soundfile as sf\n","import time\n","from scipy.spatial import distance\n","from scipy import signal\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import pickle\n","from pathlib import Path\n","from typing import *\n","import librosa\n","from scipy.ndimage import binary_dilation\n","import struct\n","from torchaudio.transforms import Resample\n","import torchaudio.transforms as trans\n","from s3prl.upstream.interfaces import UpstreamBase\n","from packaging import version\n","from fairseq import tasks\n","from fairseq.checkpoint_utils import load_checkpoint_to_cpu\n","from fairseq.dataclass.utils import convert_namespace_to_omegaconf\n","from torch.nn.utils.rnn import pad_sequence\n","from omegaconf import OmegaConf"],"metadata":{"id":"yZlOe8iUcd3-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_path = \"/content/gdrive/MyDrive/KLTN/dataset/zalo_dataset/dataset_fix/train\"\n","train_list = \"/content/gdrive/MyDrive/KLTN/dataset/zalo_dataset/dataset_fix/train_list.txt\"\n","\n","eval_path = \"/content/gdrive/MyDrive/KLTN/dataset/zalo_dataset/dataset_fix/val\"\n","eval_list = \"/content/gdrive/MyDrive/KLTN/dataset/zalo_dataset/dataset_fix/veri_val.txt\"\n","\n","save_path = \"/content/gdrive/MyDrive/KLTN/source/wavlm_large/component\""],"metadata":{"id":"3dd3TMisAslc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training setting batch30 for wavlm unfreeze, batch150 for ecapatdnn\n","n_cpu = 2\n","num_frames = 300 # 300 for 3 seconds\n","max_epoch = 20\n","batch_size = 150\n","# batch_size = 30\n","lr = 0.001\n","lr_decay = 0.97\n","test_step = 1\n","epoch = 48 # Init epoch\n","\n","# Model setting\n","n_class = 5994 # Default n_speakers\n","C = 1024 # Channel size\n","m = 0.2 # Loss margin\n","s = 32 # Loss scale"],"metadata":{"id":"u34-n9U9C6QJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# https://github.com/TaoRuijie/ECAPA-TDNN\n","import glob, numpy, os, random, soundfile, torch\n","from scipy import signal\n","\n","class train_loader(object):\n","\tdef __init__(self, train_list, train_path, num_frames, musan_path = None, rir_path = None, **kwargs):\n","\t\tself.train_path = train_path\n","\t\tself.num_frames = num_frames\n","\t\t# Load and configure augmentation files\n","\t\t# self.noisetypes = ['noise','speech','music']\n","\t\t# self.noisesnr = {'noise':[0,15],'speech':[13,20],'music':[5,15]}\n","\t\t# self.numnoise = {'noise':[1,1], 'speech':[3,8], 'music':[1,1]}\n","\t\t# self.noiselist = {}\n","\t\t# augment_files   = glob.glob(os.path.join(musan_path,'*/*/*/*.wav'))\n","\t\t# for file in augment_files:\n","\t\t# \tif file.split('/')[-4] not in self.noiselist:\n","\t\t# \t\tself.noiselist[file.split('/')[-4]] = []\n","\t\t# \tself.noiselist[file.split('/')[-4]].append(file)\n","\t\t# self.rir_files  = glob.glob(os.path.join(rir_path,'*/*/*.wav'))\n","\t\t# Load data & labels\n","\t\tself.data_list  = []\n","\t\tself.data_label = []\n","\t\tlines = open(train_list).read().splitlines()\n","\t\tdictkeys = list(set([x.split()[0] for x in lines]))\n","\t\tdictkeys.sort()\n","\t\tdictkeys = { key : ii for ii, key in enumerate(dictkeys) }\n","\t\tfor index, line in enumerate(lines):\n","\t\t\tspeaker_label = dictkeys[line.split()[0]]\n","\t\t\tfile_name     = os.path.join(train_path, line.split()[1])\n","\t\t\tself.data_label.append(speaker_label)\n","\t\t\tself.data_list.append(file_name)\n","\n","\tdef __getitem__(self, index):\n","\t\t# Read the utterance and randomly select the segment\n","\t\taudio, sr = soundfile.read(self.data_list[index])\n","\t\tlength = self.num_frames * 160 + 240\n","\t\tif audio.shape[0] <= length:\n","\t\t\tshortage = length - audio.shape[0]\n","\t\t\taudio = numpy.pad(audio, (0, shortage), 'wrap')\n","\t\tstart_frame = numpy.int64(random.random()*(audio.shape[0]-length))\n","\t\taudio = audio[start_frame:start_frame + length]\n","\t\taudio = numpy.stack([audio],axis=0)\n","\t\t# Data Augmentation\n","\t\t# augtype = random.randint(0,5)\n","\t\t# if augtype == 0:   # Original\n","\t\t# \taudio = audio\n","\t\t# elif augtype == 1: # Reverberation\n","\t\t# \taudio = self.add_rev(audio)\n","\t\t# elif augtype == 2: # Babble\n","\t\t# \taudio = self.add_noise(audio, 'speech')\n","\t\t# elif augtype == 3: # Music\n","\t\t# \taudio = self.add_noise(audio, 'music')\n","\t\t# elif augtype == 4: # Noise\n","\t\t# \taudio = self.add_noise(audio, 'noise')\n","\t\t# elif augtype == 5: # Television noise\n","\t\t# \taudio = self.add_noise(audio, 'speech')\n","\t\t# \taudio = self.add_noise(audio, 'music')\n","\t\treturn torch.FloatTensor(audio[0]), self.data_label[index]\n","\n","\tdef __len__(self):\n","\t\treturn len(self.data_list)\n","\n","\t# def add_rev(self, audio):\n","\t# \trir_file    = random.choice(self.rir_files)\n","\t# \trir, sr     = soundfile.read(rir_file)\n","\t# \trir         = numpy.expand_dims(rir.astype(numpy.float),0)\n","\t# \trir         = rir / numpy.sqrt(numpy.sum(rir**2))\n","\t# \treturn signal.convolve(audio, rir, mode='full')[:,:self.num_frames * 160 + 240]\n","\n","\t# def add_noise(self, audio, noisecat):\n","\t# \tclean_db    = 10 * numpy.log10(numpy.mean(audio ** 2)+1e-4)\n","\t# \tnumnoise    = self.numnoise[noisecat]\n","\t# \tnoiselist   = random.sample(self.noiselist[noisecat], random.randint(numnoise[0],numnoise[1]))\n","\t# \tnoises = []\n","\t# \tfor noise in noiselist:\n","\t# \t\tnoiseaudio, sr = soundfile.read(noise)\n","\t# \t\tlength = self.num_frames * 160 + 240\n","\t# \t\tif noiseaudio.shape[0] <= length:\n","\t# \t\t\tshortage = length - noiseaudio.shape[0]\n","\t# \t\t\tnoiseaudio = numpy.pad(noiseaudio, (0, shortage), 'wrap')\n","\t# \t\tstart_frame = numpy.int64(random.random()*(noiseaudio.shape[0]-length))\n","\t# \t\tnoiseaudio = noiseaudio[start_frame:start_frame + length]\n","\t# \t\tnoiseaudio = numpy.stack([noiseaudio],axis=0)\n","\t# \t\tnoise_db = 10 * numpy.log10(numpy.mean(noiseaudio ** 2)+1e-4)\n","\t# \t\tnoisesnr   = random.uniform(self.noisesnr[noisecat][0],self.noisesnr[noisecat][1])\n","\t# \t\tnoises.append(numpy.sqrt(10 ** ((clean_db - noise_db - noisesnr) / 10)) * noiseaudio)\n","\t# \tnoise = numpy.sum(numpy.concatenate(noises,axis=0),axis=0,keepdims=True)\n","\t# \treturn noise + audio"],"metadata":{"id":"DJxby5TT9gyb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainloader  = train_loader(train_list, train_path, num_frames)\n","trainLoader = torch.utils.data.DataLoader(trainloader, batch_size = batch_size, shuffle = True, num_workers = n_cpu, pin_memory=False, drop_last = False)"],"metadata":{"id":"SZRMN2kNCraH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_model(filepath):\n","    state = torch.load(filepath, map_location=lambda storage, loc: storage)\n","    # state = load_checkpoint_to_cpu(filepath)\n","    state[\"cfg\"] = OmegaConf.create(state[\"cfg\"])\n","\n","    if \"args\" in state and state[\"args\"] is not None:\n","        cfg = convert_namespace_to_omegaconf(state[\"args\"])\n","    elif \"cfg\" in state and state[\"cfg\"] is not None:\n","        cfg = state[\"cfg\"]\n","    else:\n","        raise RuntimeError(\n","            f\"Neither args nor cfg exist in state keys = {state.keys()}\"\n","            )\n","\n","    task = tasks.setup_task(cfg.task)\n","    if \"task_state\" in state:\n","        task.load_state_dict(state[\"task_state\"])\n","\n","    model = task.build_model(cfg.model)\n","\n","    return model, cfg, task\n","\n","class UpstreamExpert(UpstreamBase):\n","    def __init__(self, ckpt, **kwargs):\n","        super().__init__(**kwargs)\n","        assert version.parse(fairseq.__version__) > version.parse(\n","            \"0.10.2\"\n","        ), \"Please install the fairseq master branch.\"\n","\n","        model, cfg, task = load_model(ckpt)\n","        self.model = model\n","        self.task = task\n","\n","        if len(self.hooks) == 0:\n","            module_name = \"self.model.encoder.layers\"\n","            for module_id in range(len(eval(module_name))):\n","                self.add_hook(\n","                    f\"{module_name}[{module_id}]\",\n","                    lambda input, output: input[0].transpose(0, 1),\n","                )\n","            self.add_hook(\"self.model.encoder\", lambda input, output: output[0])\n","\n","    def forward(self, wavs):\n","        if self.task.cfg.normalize:\n","            wavs = [F.layer_norm(wav, wav.shape) for wav in wavs]\n","\n","        device = wavs[0].device\n","        wav_lengths = torch.LongTensor([len(wav) for wav in wavs]).to(device)\n","        wav_padding_mask = ~torch.lt(\n","            torch.arange(max(wav_lengths)).unsqueeze(0).to(device),\n","            wav_lengths.unsqueeze(1),\n","        )\n","        padded_wav = pad_sequence(wavs, batch_first=True)\n","\n","        features, feat_padding_mask = self.model.extract_features(\n","            padded_wav,\n","            padding_mask=wav_padding_mask,\n","            mask=None,\n","        )\n","        return {\n","            \"default\": features,\n","        }"],"metadata":{"id":"H_bqPQdSSYts"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def accuracy(output, target, topk=(1,)):\n","    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n","    maxk = max(topk)\n","    batch_size = target.size(0)\n","\n","    _, pred = output.topk(maxk, 1, True, True)\n","    pred = pred.t()\n","    correct = pred.eq(target.view(1, -1).expand_as(pred))\n","\n","    res = []\n","    for k in topk:\n","        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n","        res.append(correct_k.mul_(100.0 / batch_size))\n","    return res"],"metadata":{"id":"zgjLXlHEFVMk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","AAMsoftmax loss function copied from voxceleb_trainer: https://github.com/clovaai/voxceleb_trainer/blob/master/loss/aamsoftmax.py\n","'''\n","\n","import torch, math\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class AAMsoftmax(nn.Module):\n","    def __init__(self, n_class, m, s):\n","\n","        super(AAMsoftmax, self).__init__()\n","        self.m = m\n","        self.s = s\n","        self.weight = torch.nn.Parameter(torch.FloatTensor(n_class, 256), requires_grad=True)\n","        self.ce = nn.CrossEntropyLoss()\n","        nn.init.xavier_normal_(self.weight, gain=1)\n","        self.cos_m = math.cos(self.m)\n","        self.sin_m = math.sin(self.m)\n","        self.th = math.cos(math.pi - self.m)\n","        self.mm = math.sin(math.pi - self.m) * self.m\n","\n","    def forward(self, x, label=None):\n","\n","        cosine = F.linear(F.normalize(x), F.normalize(self.weight))\n","        sine = torch.sqrt((1.0 - torch.mul(cosine, cosine)).clamp(0, 1))\n","        phi = cosine * self.cos_m - sine * self.sin_m\n","        phi = torch.where((cosine - self.th) > 0, phi, cosine - self.mm)\n","        one_hot = torch.zeros_like(cosine)\n","        one_hot.scatter_(1, label.view(-1, 1), 1)\n","        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n","        output = output * self.s\n","\n","        loss = self.ce(output, label)\n","        prec1 = accuracy(output.detach(), label.detach(), topk=(1,))[0]\n","\n","        return loss, prec1"],"metadata":{"id":"5wDRZFKG6I0g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"euZ5jFWqhgIL"}},{"cell_type":"code","source":["class Conv1dReluBn(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=True):\n","        super().__init__()\n","        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, dilation, bias=bias)\n","        self.bn = nn.BatchNorm1d(out_channels)\n","\n","    def forward(self, x):\n","        return self.bn(F.relu(self.conv(x)))\n","\n","class Res2Conv1dReluBn(nn.Module):\n","    '''\n","    in_channels == out_channels == channels\n","    '''\n","\n","    def __init__(self, channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=True, scale=4):\n","        super().__init__()\n","        assert channels % scale == 0, \"{} % {} != 0\".format(channels, scale)\n","        self.scale = scale\n","        self.width = channels // scale\n","        self.nums = scale if scale == 1 else scale - 1\n","\n","        self.convs = []\n","        self.bns = []\n","        for i in range(self.nums):\n","            self.convs.append(nn.Conv1d(self.width, self.width, kernel_size, stride, padding, dilation, bias=bias))\n","            self.bns.append(nn.BatchNorm1d(self.width))\n","        self.convs = nn.ModuleList(self.convs)\n","        self.bns = nn.ModuleList(self.bns)\n","\n","    def forward(self, x):\n","        out = []\n","        spx = torch.split(x, self.width, 1)\n","        for i in range(self.nums):\n","            if i == 0:\n","                sp = spx[i]\n","            else:\n","                sp = sp + spx[i]\n","            # Order: conv -> relu -> bn\n","            sp = self.convs[i](sp)\n","            sp = self.bns[i](F.relu(sp))\n","            out.append(sp)\n","        if self.scale != 1:\n","            out.append(spx[self.nums])\n","        out = torch.cat(out, dim=1)\n","\n","        return out\n","\n","class SE_Connect(nn.Module):\n","    def __init__(self, channels, se_bottleneck_dim=128):\n","        super().__init__()\n","        self.linear1 = nn.Linear(channels, se_bottleneck_dim)\n","        self.linear2 = nn.Linear(se_bottleneck_dim, channels)\n","\n","    def forward(self, x):\n","        out = x.mean(dim=2)\n","        out = F.relu(self.linear1(out))\n","        out = torch.sigmoid(self.linear2(out))\n","        out = x * out.unsqueeze(2)\n","\n","        return out\n","\n","class SE_Res2Block(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation, scale, se_bottleneck_dim):\n","        super().__init__()\n","        self.Conv1dReluBn1 = Conv1dReluBn(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n","        self.Res2Conv1dReluBn = Res2Conv1dReluBn(out_channels, kernel_size, stride, padding, dilation, scale=scale)\n","        self.Conv1dReluBn2 = Conv1dReluBn(out_channels, out_channels, kernel_size=1, stride=1, padding=0)\n","        self.SE_Connect = SE_Connect(out_channels, se_bottleneck_dim)\n","\n","        self.shortcut = None\n","        if in_channels != out_channels:\n","            self.shortcut = nn.Conv1d(\n","                in_channels=in_channels,\n","                out_channels=out_channels,\n","                kernel_size=1,\n","            )\n","\n","    def forward(self, x):\n","        residual = x\n","        if self.shortcut:\n","            residual = self.shortcut(x)\n","\n","        x = self.Conv1dReluBn1(x)\n","        x = self.Res2Conv1dReluBn(x)\n","        x = self.Conv1dReluBn2(x)\n","        x = self.SE_Connect(x)\n","\n","        return x + residual\n","\n","class AttentiveStatsPool(nn.Module):\n","    def __init__(self, in_dim, attention_channels=128, global_context_att=False):\n","        super().__init__()\n","        self.global_context_att = global_context_att\n","\n","        # Use Conv1d with stride == 1 rather than Linear, then we don't need to transpose inputs.\n","        if global_context_att:\n","            self.linear1 = nn.Conv1d(in_dim * 3, attention_channels, kernel_size=1)  # equals W and b in the paper\n","        else:\n","            self.linear1 = nn.Conv1d(in_dim, attention_channels, kernel_size=1)  # equals W and b in the paper\n","        self.linear2 = nn.Conv1d(attention_channels, in_dim, kernel_size=1)  # equals V and k in the paper\n","\n","    def forward(self, x):\n","\n","        if self.global_context_att:\n","            context_mean = torch.mean(x, dim=-1, keepdim=True).expand_as(x)\n","            context_std = torch.sqrt(torch.var(x, dim=-1, keepdim=True) + 1e-10).expand_as(x)\n","            x_in = torch.cat((x, context_mean, context_std), dim=1)\n","        else:\n","            x_in = x\n","\n","        # DON'T use ReLU here! In experiments, I find ReLU hard to converge.\n","        alpha = torch.tanh(self.linear1(x_in))\n","        # alpha = F.relu(self.linear1(x_in))\n","        alpha = torch.softmax(self.linear2(alpha), dim=2)\n","        mean = torch.sum(alpha * x, dim=2)\n","        residuals = torch.sum(alpha * (x ** 2), dim=2) - mean ** 2\n","        std = torch.sqrt(residuals.clamp(min=1e-9))\n","        return torch.cat([mean, std], dim=1)\n","\n","class ECAPA_TDNN(nn.Module):\n","    def __init__(self, feat_dim=80, channels=512, emb_dim=192, global_context_att=False,\n","                 feat_type='fbank', sr=16000, feature_selection=\"hidden_states\", update_extract=False, config_path=None):\n","        super().__init__()\n","\n","        self.feat_type = feat_type\n","        self.feature_selection = feature_selection\n","        self.update_extract = update_extract\n","        self.sr = sr\n","\n","        if feat_type == \"fbank\" or feat_type == \"mfcc\":\n","            self.update_extract = False\n","\n","        win_len = int(sr * 0.025)\n","        hop_len = int(sr * 0.01)\n","\n","        if feat_type == 'fbank':\n","            self.feature_extract = trans.MelSpectrogram(sample_rate=sr, n_fft=512, win_length=win_len,\n","                                                        hop_length=hop_len, f_min=0.0, f_max=sr // 2,\n","                                                        pad=0, n_mels=feat_dim)\n","        elif feat_type == 'mfcc':\n","            melkwargs = {\n","                'n_fft': 512,\n","                'win_length': win_len,\n","                'hop_length': hop_len,\n","                'f_min': 0.0,\n","                'f_max': sr // 2,\n","                'pad': 0\n","            }\n","            self.feature_extract = trans.MFCC(sample_rate=sr, n_mfcc=feat_dim, log_mels=False,\n","                                              melkwargs=melkwargs)\n","        else:\n","            if config_path is None:\n","                self.feature_extract = torch.hub.load('s3prl/s3prl', feat_type)\n","            else:\n","                self.feature_extract = UpstreamExpert(config_path)\n","            if len(self.feature_extract.model.encoder.layers) == 24 and hasattr(self.feature_extract.model.encoder.layers[23].self_attn, \"fp32_attention\"):\n","                self.feature_extract.model.encoder.layers[23].self_attn.fp32_attention = False\n","            if len(self.feature_extract.model.encoder.layers) == 24 and hasattr(self.feature_extract.model.encoder.layers[11].self_attn, \"fp32_attention\"):\n","                self.feature_extract.model.encoder.layers[11].self_attn.fp32_attention = False\n","\n","            self.feat_num = self.get_feat_num()\n","            self.feature_weight = nn.Parameter(torch.zeros(self.feat_num))\n","\n","        if feat_type != 'fbank' and feat_type != 'mfcc':\n","            freeze_list = ['final_proj', 'label_embs_concat', 'mask_emb', 'project_q', 'quantizer']\n","            for name, param in self.feature_extract.named_parameters():\n","                for freeze_val in freeze_list:\n","                    if freeze_val in name:\n","                        param.requires_grad = False\n","                        break\n","\n","        if not self.update_extract:\n","            for param in self.feature_extract.parameters():\n","                param.requires_grad = False\n","\n","        self.instance_norm = nn.InstanceNorm1d(feat_dim)\n","        # self.channels = [channels] * 4 + [channels * 3]\n","        self.channels = [channels] * 4 + [1536]\n","\n","        self.layer1 = Conv1dReluBn(feat_dim, self.channels[0], kernel_size=5, padding=2)\n","        self.layer2 = SE_Res2Block(self.channels[0], self.channels[1], kernel_size=3, stride=1, padding=2, dilation=2, scale=8, se_bottleneck_dim=128)\n","        self.layer3 = SE_Res2Block(self.channels[1], self.channels[2], kernel_size=3, stride=1, padding=3, dilation=3, scale=8, se_bottleneck_dim=128)\n","        self.layer4 = SE_Res2Block(self.channels[2], self.channels[3], kernel_size=3, stride=1, padding=4, dilation=4, scale=8, se_bottleneck_dim=128)\n","\n","        # self.conv = nn.Conv1d(self.channels[-1], self.channels[-1], kernel_size=1)\n","        cat_channels = channels * 3\n","        self.conv = nn.Conv1d(cat_channels, self.channels[-1], kernel_size=1)\n","        self.pooling = AttentiveStatsPool(self.channels[-1], attention_channels=128, global_context_att=global_context_att)\n","        self.bn = nn.BatchNorm1d(self.channels[-1] * 2)\n","        self.linear = nn.Linear(self.channels[-1] * 2, emb_dim)\n","\n","        # Component part\n","        self.new_component = nn.Sequential(\n","            nn.Linear(256, 256),\n","            nn.Linear(256, 256),\n","            nn.Linear(256, 256),\n","        )\n","\n","    def get_feat_num(self):\n","        self.feature_extract.eval()\n","        wav = [torch.randn(self.sr).to(next(self.feature_extract.parameters()).device)]\n","        with torch.no_grad():\n","            features = self.feature_extract(wav)\n","        select_feature = features[self.feature_selection]\n","        if isinstance(select_feature, (list, tuple)):\n","            return len(select_feature)\n","        else:\n","            return 1\n","\n","    def get_feat(self, x):\n","        if self.update_extract:\n","            x = self.feature_extract([sample for sample in x])\n","        else:\n","            with torch.no_grad():\n","                if self.feat_type == 'fbank' or self.feat_type == 'mfcc':\n","                    x = self.feature_extract(x) + 1e-6  # B x feat_dim x time_len\n","                else:\n","                    x = self.feature_extract([sample for sample in x])\n","\n","        if self.feat_type == 'fbank':\n","            x = x.log()\n","\n","        if self.feat_type != \"fbank\" and self.feat_type != \"mfcc\":\n","            x = x[self.feature_selection]\n","            if isinstance(x, (list, tuple)):\n","                x = torch.stack(x, dim=0)\n","            else:\n","                x = x.unsqueeze(0)\n","            norm_weights = F.softmax(self.feature_weight, dim=-1).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n","            x = (norm_weights * x).sum(dim=0)\n","            x = torch.transpose(x, 1, 2) + 1e-6\n","\n","        x = self.instance_norm(x)\n","        return x\n","\n","    def forward(self, x):\n","        x = self.get_feat(x)\n","\n","        out1 = self.layer1(x)\n","        out2 = self.layer2(out1)\n","        out3 = self.layer3(out2)\n","        out4 = self.layer4(out3)\n","\n","        out = torch.cat([out2, out3, out4], dim=1)\n","        out = F.relu(self.conv(out))\n","        out = self.bn(self.pooling(out))\n","        out = self.linear(out)\n","\n","        out = self.new_component(out)\n","\n","        return out\n","\n","\n","def ECAPA_TDNN_SMALL(feat_dim, emb_dim=256, feat_type='fbank', sr=16000, feature_selection=\"hidden_states\", update_extract=False, config_path=None):\n","    return ECAPA_TDNN(feat_dim=feat_dim, channels=512, emb_dim=emb_dim,\n","                      feat_type=feat_type, sr=sr, feature_selection=feature_selection, update_extract=update_extract, config_path=config_path)"],"metadata":{"id":"2BGaCC4vSBEP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# update_extract=True\n","# model = ECAPA_TDNN_SMALL(feat_dim=1024, feat_type='wavlm_large', config_path=config_path, update_extract=True)\n","MODEL_LIST = ['ecapa_tdnn', 'hubert_large', 'wav2vec2_xlsr', 'unispeech_sat', \"wavlm_base_plus\", \"wavlm_large\"]\n","\n","def init_model(model_name, checkpoint=None, update_component=False):\n","    if model_name == 'unispeech_sat':\n","        config_path = const_path + 'VerificationVoxCeleb1/unispeech_sat.th'\n","        model = ECAPA_TDNN_SMALL(feat_dim=1024, feat_type='unispeech_sat', config_path=config_path)\n","    elif model_name == 'wavlm_base_plus':\n","        config_path = None\n","        model = ECAPA_TDNN_SMALL(feat_dim=768, feat_type='wavlm_base_plus', config_path=config_path)\n","    elif model_name == 'wavlm_large':\n","        config_path = None\n","        model = ECAPA_TDNN_SMALL(feat_dim=1024, feat_type='wavlm_large', config_path=config_path)\n","        if update_component==True:\n","          for param in model.parameters():\n","            param.requires_grad = False\n","          for param in model.new_component.parameters():\n","            param.requires_grad = True\n","\n","    elif model_name == 'hubert_large':\n","        config_path = None\n","        model = ECAPA_TDNN_SMALL(feat_dim=1024, feat_type='hubert_large_ll60k', config_path=config_path)\n","    elif model_name == 'wav2vec2_xlsr':\n","        config_path = None\n","        model = ECAPA_TDNN_SMALL(feat_dim=1024, feat_type='wav2vec2_xlsr', config_path=config_path)\n","    else:\n","        model = ECAPA_TDNN_SMALL(feat_dim=40, feat_type='fbank')\n","\n","    if checkpoint is not None:\n","        state_dict = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n","        model.load_state_dict(state_dict['model'], strict=False)\n","    return model"],"metadata":{"id":"5YojOet2QcUF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Some utilized functions\n","These functions are all copied from voxceleb_trainer: https://github.com/clovaai/voxceleb_trainer/blob/master/tuneThreshold.py\n","'''\n","\n","import os, numpy, torch\n","from sklearn import metrics\n","from operator import itemgetter\n","import torch.nn.functional as F\n","\n","\n","def tuneThresholdfromScore(scores, labels, target_fa, target_fr = None):\n","\n","\tfpr, tpr, thresholds = metrics.roc_curve(labels, scores, pos_label=1)\n","\tfnr = 1 - tpr\n","\ttunedThreshold = [];\n","\tif target_fr:\n","\t\tfor tfr in target_fr:\n","\t\t\tidx = numpy.nanargmin(numpy.absolute((tfr - fnr)))\n","\t\t\ttunedThreshold.append([thresholds[idx], fpr[idx], fnr[idx]])\n","\tfor tfa in target_fa:\n","\t\tidx = numpy.nanargmin(numpy.absolute((tfa - fpr))) # numpy.where(fpr<=tfa)[0][-1]\n","\t\ttunedThreshold.append([thresholds[idx], fpr[idx], fnr[idx]])\n","\tidxE = numpy.nanargmin(numpy.absolute((fnr - fpr)))\n","\teer  = max(fpr[idxE],fnr[idxE])*100\n","\n","\treturn tunedThreshold, eer, fpr, fnr\n","\n","# Creates a list of false-negative rates, a list of false-positive rates\n","# and a list of decision thresholds that give those error-rates.\n","def ComputeErrorRates(scores, labels):\n","\n","      # Sort the scores from smallest to largest, and also get the corresponding\n","      # indexes of the sorted scores.  We will treat the sorted scores as the\n","      # thresholds at which the the error-rates are evaluated.\n","      sorted_indexes, thresholds = zip(*sorted(\n","          [(index, threshold) for index, threshold in enumerate(scores)],\n","          key=itemgetter(1)))\n","      sorted_labels = []\n","      labels = [labels[i] for i in sorted_indexes]\n","      fnrs = []\n","      fprs = []\n","\n","      # At the end of this loop, fnrs[i] is the number of errors made by\n","      # incorrectly rejecting scores less than thresholds[i]. And, fprs[i]\n","      # is the total number of times that we have correctly accepted scores\n","      # greater than thresholds[i].\n","      for i in range(0, len(labels)):\n","          if i == 0:\n","              fnrs.append(labels[i])\n","              fprs.append(1 - labels[i])\n","          else:\n","              fnrs.append(fnrs[i-1] + labels[i])\n","              fprs.append(fprs[i-1] + 1 - labels[i])\n","      fnrs_norm = sum(labels)\n","      fprs_norm = len(labels) - fnrs_norm\n","\n","      # Now divide by the total number of false negative errors to\n","      # obtain the false positive rates across all thresholds\n","      fnrs = [x / float(fnrs_norm) for x in fnrs]\n","\n","      # Divide by the total number of corret positives to get the\n","      # true positive rate.  Subtract these quantities from 1 to\n","      # get the false positive rates.\n","      fprs = [1 - x / float(fprs_norm) for x in fprs]\n","      return fnrs, fprs, thresholds\n","\n","# Computes the minimum of the detection cost function.  The comments refer to\n","# equations in Section 3 of the NIST 2016 Speaker Recognition Evaluation Plan.\n","def ComputeMinDcf(fnrs, fprs, thresholds, p_target, c_miss, c_fa):\n","    min_c_det = float(\"inf\")\n","    min_c_det_threshold = thresholds[0]\n","    for i in range(0, len(fnrs)):\n","        # See Equation (2).  it is a weighted sum of false negative\n","        # and false positive errors.\n","        c_det = c_miss * fnrs[i] * p_target + c_fa * fprs[i] * (1 - p_target)\n","        if c_det < min_c_det:\n","            min_c_det = c_det\n","            min_c_det_threshold = thresholds[i]\n","    # See Equations (3) and (4).  Now we normalize the cost.\n","    c_def = min(c_miss * p_target, c_fa * (1 - p_target))\n","    min_dcf = min_c_det / c_def\n","    return min_dcf, min_c_det_threshold\n","\n","def accuracy(output, target, topk=(1,)):\n","\n","\tmaxk = max(topk)\n","\tbatch_size = target.size(0)\n","\t_, pred = output.topk(maxk, 1, True, True)\n","\tpred = pred.t()\n","\tcorrect = pred.eq(target.view(1, -1).expand_as(pred))\n","\tres = []\n","\tfor k in topk:\n","\t\tcorrect_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n","\t\tres.append(correct_k.mul_(100.0 / batch_size))\n","\n","\treturn res"],"metadata":{"id":"Of1m2ryILnpv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from scipy import signal\n","import matplotlib.pyplot as plt\n","import numpy as np\n","# Using optim SGD\n","class ECAPAModel(nn.Module):\n","\tdef __init__(self, lr, lr_decay, C , n_class, m, s, test_step, **kwargs):\n","\t\tsuper(ECAPAModel, self).__init__()\n","\t\t## ECAPA-TDNN\n","\t\tself.speaker_encoder = (init_model('wavlm_large', const_path + 'KLTN/source/wavlm_large/wavlm_large_finetune.pth',update_component=True)).cuda()\n","\t\t## Classifier\n","\t\tself.speaker_loss    = AAMsoftmax(n_class = n_class, m = m, s = s).cuda()\n","\n","\t\tself.optim           = torch.optim.SGD(self.parameters(), lr = lr, weight_decay = 2e-5)\n","\t\tself.scheduler       = torch.optim.lr_scheduler.StepLR(self.optim, step_size = test_step, gamma=lr_decay)\n","\t\tprint(time.strftime(\"%m-%d %H:%M:%S\") + \" Model para number = %.2f\"%(sum(param.numel() for param in self.speaker_encoder.parameters())))\n","\n","\tdef train_network(self, epoch, loader):\n","\t\tself.train()\n","\t\t## Update the learning rate based on the current epcoh\n","\t\tself.scheduler.step(epoch - 1)\n","\t\tindex, top1, loss = 0, 0, 0\n","\t\tlr = self.optim.param_groups[0]['lr']\n","\t\tfor num, (data, labels) in tqdm.tqdm(enumerate(loader, start = 1)):\n","\t\t\tself.zero_grad()\n","\t\t\tlabels            = torch.LongTensor(labels).cuda()\n","\t\t\tspeaker_embedding = self.speaker_encoder.forward(data.cuda())\n","\t\t\tnloss, prec       = self.speaker_loss.forward(speaker_embedding, labels)\n","\t\t\tnloss.backward()\n","\t\t\tself.optim.step()\n","\t\t\tindex += len(labels)\n","\t\t\ttop1 += prec\n","\t\t\tloss += nloss.detach().cpu().numpy()\n","\t\t\tprint(time.strftime(\"%m-%d %H:%M:%S\") + \\\n","\t\t\t\" [%2d] Lr: %5f, Training: %.2f%%, \"    %(epoch, lr, 100 * (num / loader.__len__())) + \\\n","\t\t\t\" Loss: %.5f, ACC: %2.2f%% \\r\"        %(loss/(num), top1/index*len(labels)))\n","\t\t\t# sys.stderr.flush()\n","\t\tprint(\"\\n\")\n","\t\treturn loss/num, lr, top1/index*len(labels)\n","\n","\tdef eval_network(self, eval_list, eval_path):\n","\t\tself.eval()\n","\t\tfiles = []\n","\t\tEERs = []\n","\t\tembeddings = {}\n","\t\tlines = open(eval_list).read().splitlines()\n","\t\tfor line in lines:\n","\t\t\tfiles.append(line.split()[1])\n","\t\t\tfiles.append(line.split()[2])\n","\t\tsetfiles = list(set(files))\n","\t\tsetfiles.sort()\n","\n","\t\t# filter_type = [\"lowpass\"]\n","\t\t# filter = \"lowpass\"\n","\t\t# fc_arr = [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 5500, 6000, 6500, 7000, 7500]\n","\t\t# order = 10\n","\n","\t\t# for (y, fc) in enumerate(fc_arr[:-2] if filter == \"bandpass\" else fc_arr):\n","\t\tfor idx, file in tqdm.tqdm(enumerate(setfiles), total = len(setfiles)):\n","\n","\t\t\taudio, _  = soundfile.read(os.path.join(eval_path, file))\n","\n","\t\t\t# b, a = signal.butter(order, fc/(16000/2), btype=\"lowpass\", analog=False)\n","\t\t\t# audio = signal.filtfilt(b, a, audio)\n","\t\t\t# Full utterance\n","\t\t\tdata_1 = torch.FloatTensor(numpy.stack([audio],axis=0)).cuda()\n","\n","\t\t\t# Spliited utterance matrix\n","\t\t\tmax_audio = 300 * 160 + 240\n","\t\t\tif audio.shape[0] <= max_audio:\n","\t\t\t\tshortage = max_audio - audio.shape[0]\n","\t\t\t\taudio = numpy.pad(audio, (0, shortage), 'wrap')\n","\t\t\tfeats = []\n","\t\t\tstartframe = numpy.linspace(0, audio.shape[0]-max_audio, num=5)\n","\t\t\tfor asf in startframe:\n","\t\t\t\tfeats.append(audio[int(asf):int(asf)+max_audio])\n","\t\t\tfeats = numpy.stack(feats, axis = 0).astype(numpy.float)\n","\t\t\tdata_2 = torch.FloatTensor(feats).cuda()\n","\t\t\t# Speaker embeddings\n","\t\t\twith torch.no_grad():\n","\t\t\t\tembedding_1 = self.speaker_encoder.forward(data_1)\n","\t\t\t\tembedding_1 = F.normalize(embedding_1, p=2, dim=1)\n","\t\t\t\tembedding_2 = self.speaker_encoder.forward(data_2)\n","\t\t\t\tembedding_2 = F.normalize(embedding_2, p=2, dim=1)\n","\t\t\tembeddings[file] = [embedding_1, embedding_2]\n","\t\tscores, labels  = [], []\n","\n","\t\tfor line in lines:\n","\t\t\tembedding_11, embedding_12 = embeddings[line.split()[1]]\n","\t\t\tembedding_21, embedding_22 = embeddings[line.split()[2]]\n","\t\t\t# Compute the scores\n","\t\t\tscore_1 = torch.mean(torch.matmul(embedding_11, embedding_21.T)) # higher is positive\n","\t\t\tscore_2 = torch.mean(torch.matmul(embedding_12, embedding_22.T))\n","\t\t\tscore = (score_1 + score_2) / 2\n","\t\t\tscore = score.detach().cpu().numpy()\n","\t\t\tscores.append(score)\n","\t\t\tlabels.append(int(line.split()[0]))\n","\n","\t\t# Coumpute EER and minDCF\n","\t\tEER = tuneThresholdfromScore(scores, labels, [1, 0.1])[1]\n","\t\tfnrs, fprs, thresholds = ComputeErrorRates(scores, labels)\n","\t\tminDCF, _ = ComputeMinDcf(fnrs, fprs, thresholds, 0.05, 1, 1)\n","\t\tEERs.append(EER)\n","\t \t\t# print(EER)\n","\t\treturn EERs, minDCF\n","\n","\tdef save_parameters(self, path):\n","\t\ttorch.save(self.state_dict(), path)\n","\n","\tdef load_parameters(self, path):\n","\t\tself_state = self.state_dict()\n","\t\tloaded_state = torch.load(path)\n","\t\tfor name, param in loaded_state.items():\n","\t\t\torigname = name\n","\t\t\tif name not in self_state:\n","\t\t\t\tname = name.replace(\"module.\", \"\")\n","\t\t\t\tif name not in self_state:\n","\t\t\t\t\tprint(\"%s is not in the model.\"%origname)\n","\t\t\t\t\tcontinue\n","\t\t\tif self_state[name].size() != loaded_state[origname].size():\n","\t\t\t\tprint(\"Wrong parameter length: %s, model: %s, loaded: %s\"%(origname, self_state[name].size(), loaded_state[origname].size()))\n","\t\t\t\tcontinue\n","\t\t\tself_state[name].copy_(param)"],"metadata":{"id":"exxG17f_F4CM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"d4gQ_x6gm9XZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# order = 10"],"metadata":{"id":"iIwVfY4pmvQK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = ECAPAModel(lr, lr_decay, C , n_class, m, s, test_step)"],"metadata":{"id":"A32eegykLt_J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687374804199,"user_tz":-420,"elapsed":19541,"user":{"displayName":"Tường Đinh","userId":"01095438614226568020"}},"outputId":"f462661c-7ccf-4373-94d5-ea1da95811a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/s3prl_s3prl_main\n"]},{"output_type":"stream","name":"stdout","text":["06-21 19:13:29 Model para number = 324061337.00\n"]}]},{"cell_type":"code","source":["model.load_parameters(\"/content/gdrive/MyDrive/KLTN/source/wavlm_large/component/model_0049.model\")\n","# model.speaker_encoder.eval()"],"metadata":{"id":"Id3SRaXPBdHd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result,_ = model.eval_network(eval_list, eval_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fuMP-yfaUBS4","executionInfo":{"status":"ok","timestamp":1687379467991,"user_tz":-420,"elapsed":4657306,"user":{"displayName":"Tường Đinh","userId":"01095438614226568020"}},"outputId":"bbba0b1c-71e9-45f6-a66a-3f51634dad1f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/973 [00:00<?, ?it/s]<ipython-input-37-af426667ac6e>:74: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  feats = numpy.stack(feats, axis = 0).astype(numpy.float)\n","100%|██████████| 973/973 [14:50<00:00,  1.09it/s]\n","  0%|          | 0/973 [00:00<?, ?it/s]<ipython-input-37-af426667ac6e>:74: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  feats = numpy.stack(feats, axis = 0).astype(numpy.float)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","100%|██████████| 973/973 [04:28<00:00,  3.62it/s]\n","  0%|          | 0/973 [00:00<?, ?it/s]<ipython-input-37-af426667ac6e>:74: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  feats = numpy.stack(feats, axis = 0).astype(numpy.float)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","100%|██████████| 973/973 [04:26<00:00,  3.65it/s]\n","  0%|          | 0/973 [00:00<?, ?it/s]<ipython-input-37-af426667ac6e>:74: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  feats = numpy.stack(feats, axis = 0).astype(numpy.float)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","100%|██████████| 973/973 [04:25<00:00,  3.66it/s]\n","  0%|          | 0/973 [00:00<?, ?it/s]<ipython-input-37-af426667ac6e>:74: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  feats = numpy.stack(feats, axis = 0).astype(numpy.float)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","100%|██████████| 973/973 [04:24<00:00,  3.68it/s]\n","  0%|          | 0/973 [00:00<?, ?it/s]<ipython-input-37-af426667ac6e>:74: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  feats = numpy.stack(feats, axis = 0).astype(numpy.float)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","100%|██████████| 973/973 [04:26<00:00,  3.65it/s]\n","  0%|          | 0/973 [00:00<?, ?it/s]<ipython-input-37-af426667ac6e>:74: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  feats = numpy.stack(feats, axis = 0).astype(numpy.float)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","100%|██████████| 973/973 [04:24<00:00,  3.68it/s]\n","  0%|          | 0/973 [00:00<?, ?it/s]<ipython-input-37-af426667ac6e>:74: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  feats = numpy.stack(feats, axis = 0).astype(numpy.float)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","100%|██████████| 973/973 [04:25<00:00,  3.66it/s]\n","  0%|          | 0/973 [00:00<?, ?it/s]<ipython-input-37-af426667ac6e>:74: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  feats = numpy.stack(feats, axis = 0).astype(numpy.float)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","100%|██████████| 973/973 [04:24<00:00,  3.68it/s]\n","  0%|          | 0/973 [00:00<?, ?it/s]<ipython-input-37-af426667ac6e>:74: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  feats = numpy.stack(feats, axis = 0).astype(numpy.float)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","100%|██████████| 973/973 [04:24<00:00,  3.67it/s]\n","  0%|          | 0/973 [00:00<?, ?it/s]<ipython-input-37-af426667ac6e>:74: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  feats = numpy.stack(feats, axis = 0).astype(numpy.float)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","100%|██████████| 973/973 [04:25<00:00,  3.67it/s]\n","  0%|          | 0/973 [00:00<?, ?it/s]<ipython-input-37-af426667ac6e>:74: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  feats = numpy.stack(feats, axis = 0).astype(numpy.float)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","100%|██████████| 973/973 [04:26<00:00,  3.66it/s]\n","  0%|          | 0/973 [00:00<?, ?it/s]<ipython-input-37-af426667ac6e>:74: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  feats = numpy.stack(feats, axis = 0).astype(numpy.float)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","100%|██████████| 973/973 [04:24<00:00,  3.68it/s]\n","  0%|          | 0/973 [00:00<?, ?it/s]<ipython-input-37-af426667ac6e>:74: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  feats = numpy.stack(feats, axis = 0).astype(numpy.float)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","100%|██████████| 973/973 [04:24<00:00,  3.68it/s]\n","  0%|          | 0/973 [00:00<?, ?it/s]<ipython-input-37-af426667ac6e>:74: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  feats = numpy.stack(feats, axis = 0).astype(numpy.float)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n","100%|██████████| 973/973 [04:25<00:00,  3.67it/s]\n"]}]},{"cell_type":"code","source":["result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ddIxxKBIUBN-","executionInfo":{"status":"ok","timestamp":1687379467992,"user_tz":-420,"elapsed":16,"user":{"displayName":"Tường Đinh","userId":"01095438614226568020"}},"outputId":"d3962142-3d7f-4df1-84cc-35b00c1b9f78"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[24.750000000000007,\n"," 16.54,\n"," 14.34,\n"," 11.97,\n"," 10.419999999999996,\n"," 9.360000000000001,\n"," 8.14,\n"," 7.580000000000001,\n"," 6.87,\n"," 6.660000000000001,\n"," 6.5,\n"," 6.399999999999995,\n"," 6.21,\n"," 5.99,\n"," 5.9200000000000035]"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":[],"metadata":{"id":"b1NnxFro_BIu"},"execution_count":null,"outputs":[]}]}